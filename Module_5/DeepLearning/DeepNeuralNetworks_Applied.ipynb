{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Off\n",
    "\n",
    "What hyperparameters do we need to tuning when training neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is an open-source neural-network library written in Python. It is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, Theano, or PlaidML. Designed to enable fast experimentation with deep neural networks, it focuses on being user-friendly, modular, and extensible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install keras with tensorflow backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://www.tensorflow.org/install/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. https://keras.io/#installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a Model with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import  Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Create first network with Keras\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "import pandas as pd\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pima indians dataset\n",
    "dataset = pd.read_csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\",header=None, delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3    4     5      6   7  8\n",
       "0  6  148  72  35    0  33.6  0.627  50  1\n",
       "1  1   85  66  29    0  26.6  0.351  31  0\n",
       "2  8  183  64   0    0  23.3  0.672  32  1\n",
       "3  1   89  66  23   94  28.1  0.167  21  0\n",
       "4  0  137  40  35  168  43.1  2.288  33  1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `train_test_split` from `sklearn.model_selection`\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Specify the data \n",
    "X = dataset.iloc[:,0:8]\n",
    "y = dataset.iloc[:,8]\n",
    "\n",
    "# Split the data up in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "Models in Keras are defined as a sequence of layers.\n",
    "\n",
    "We create a Sequential model and add layers one at a time until we are happy with our network topology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Number of Neurons per layer**\n",
    "\n",
    "The number of nuerons for the input and output layers are dependent on your data and the task. For hiddne layers, a common practice is to create a funnel with funnel with fewer and fewer neurons per layer.\n",
    "\n",
    "In general, you will get more bang for your buck by adding on more layers than adding more neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0711 10:59:44.615839 4629296576 deprecation_wrapper.py:119] From /Users/kalkidanlemma/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0711 10:59:44.630746 4629296576 deprecation_wrapper.py:119] From /Users/kalkidanlemma/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0711 10:59:44.633680 4629296576 deprecation_wrapper.py:119] From /Users/kalkidanlemma/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1-layer network\n",
    "model = Sequential()\n",
    "\n",
    "#create first hidden layer, 12 hidden layers that all connect to each of the 8 input neurons\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "#Final Layer, 1 neuron for the Target which is a binary classification so why we used sigmoid\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0711 11:01:11.468687 4629296576 deprecation_wrapper.py:119] From /Users/kalkidanlemma/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0711 11:01:11.487066 4629296576 deprecation_wrapper.py:119] From /Users/kalkidanlemma/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0711 11:01:11.491806 4629296576 deprecation.py:323] From /Users/kalkidanlemma/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#compile model, \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/50\n",
      "576/576 [==============================] - 0s 24us/step - loss: 0.6016 - acc: 0.6944 - val_loss: 0.6211 - val_acc: 0.7031\n",
      "Epoch 2/50\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.5977 - acc: 0.7049 - val_loss: 0.6125 - val_acc: 0.7187\n",
      "Epoch 3/50\n",
      "576/576 [==============================] - 0s 32us/step - loss: 0.5979 - acc: 0.6979 - val_loss: 0.6112 - val_acc: 0.7187\n",
      "Epoch 4/50\n",
      "576/576 [==============================] - 0s 27us/step - loss: 0.5970 - acc: 0.6927 - val_loss: 0.6133 - val_acc: 0.7135\n",
      "Epoch 5/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5954 - acc: 0.7031 - val_loss: 0.6146 - val_acc: 0.7083\n",
      "Epoch 6/50\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.5945 - acc: 0.7031 - val_loss: 0.6104 - val_acc: 0.7135\n",
      "Epoch 7/50\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.5945 - acc: 0.7014 - val_loss: 0.6100 - val_acc: 0.7135\n",
      "Epoch 8/50\n",
      "576/576 [==============================] - 0s 34us/step - loss: 0.5933 - acc: 0.6944 - val_loss: 0.6108 - val_acc: 0.7135\n",
      "Epoch 9/50\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.5923 - acc: 0.7083 - val_loss: 0.6130 - val_acc: 0.7240\n",
      "Epoch 10/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5915 - acc: 0.7118 - val_loss: 0.6115 - val_acc: 0.7240\n",
      "Epoch 11/50\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.5914 - acc: 0.7066 - val_loss: 0.6105 - val_acc: 0.7188\n",
      "Epoch 12/50\n",
      "576/576 [==============================] - 0s 25us/step - loss: 0.5918 - acc: 0.6927 - val_loss: 0.6104 - val_acc: 0.7240\n",
      "Epoch 13/50\n",
      "576/576 [==============================] - 0s 25us/step - loss: 0.5924 - acc: 0.7118 - val_loss: 0.6116 - val_acc: 0.7188\n",
      "Epoch 14/50\n",
      "576/576 [==============================] - 0s 22us/step - loss: 0.5884 - acc: 0.7014 - val_loss: 0.6076 - val_acc: 0.7031\n",
      "Epoch 15/50\n",
      "576/576 [==============================] - 0s 22us/step - loss: 0.5882 - acc: 0.7066 - val_loss: 0.6097 - val_acc: 0.7344\n",
      "Epoch 16/50\n",
      "576/576 [==============================] - 0s 22us/step - loss: 0.5880 - acc: 0.7031 - val_loss: 0.6079 - val_acc: 0.7292\n",
      "Epoch 17/50\n",
      "576/576 [==============================] - 0s 23us/step - loss: 0.5891 - acc: 0.7118 - val_loss: 0.6061 - val_acc: 0.7188\n",
      "Epoch 18/50\n",
      "576/576 [==============================] - 0s 23us/step - loss: 0.5871 - acc: 0.6997 - val_loss: 0.6037 - val_acc: 0.7083\n",
      "Epoch 19/50\n",
      "576/576 [==============================] - 0s 22us/step - loss: 0.5865 - acc: 0.7066 - val_loss: 0.6095 - val_acc: 0.7292\n",
      "Epoch 20/50\n",
      "576/576 [==============================] - 0s 23us/step - loss: 0.5865 - acc: 0.7014 - val_loss: 0.6044 - val_acc: 0.7188\n",
      "Epoch 21/50\n",
      "576/576 [==============================] - 0s 22us/step - loss: 0.5844 - acc: 0.7083 - val_loss: 0.6101 - val_acc: 0.7292\n",
      "Epoch 22/50\n",
      "576/576 [==============================] - 0s 23us/step - loss: 0.5845 - acc: 0.7170 - val_loss: 0.6034 - val_acc: 0.7188\n",
      "Epoch 23/50\n",
      "576/576 [==============================] - 0s 23us/step - loss: 0.5852 - acc: 0.7031 - val_loss: 0.6056 - val_acc: 0.7240\n",
      "Epoch 24/50\n",
      "576/576 [==============================] - 0s 23us/step - loss: 0.5835 - acc: 0.7083 - val_loss: 0.6014 - val_acc: 0.7083\n",
      "Epoch 25/50\n",
      "576/576 [==============================] - 0s 24us/step - loss: 0.5826 - acc: 0.7101 - val_loss: 0.6087 - val_acc: 0.7188\n",
      "Epoch 26/50\n",
      "576/576 [==============================] - 0s 21us/step - loss: 0.5821 - acc: 0.7135 - val_loss: 0.6040 - val_acc: 0.7188\n",
      "Epoch 27/50\n",
      "576/576 [==============================] - 0s 20us/step - loss: 0.5824 - acc: 0.7083 - val_loss: 0.6043 - val_acc: 0.7135\n",
      "Epoch 28/50\n",
      "576/576 [==============================] - 0s 23us/step - loss: 0.5841 - acc: 0.7049 - val_loss: 0.6026 - val_acc: 0.7240\n",
      "Epoch 29/50\n",
      "576/576 [==============================] - 0s 23us/step - loss: 0.5806 - acc: 0.7101 - val_loss: 0.6057 - val_acc: 0.7188\n",
      "Epoch 30/50\n",
      "576/576 [==============================] - 0s 22us/step - loss: 0.5815 - acc: 0.7066 - val_loss: 0.6014 - val_acc: 0.7187\n",
      "Epoch 31/50\n",
      "576/576 [==============================] - 0s 22us/step - loss: 0.5802 - acc: 0.7153 - val_loss: 0.6047 - val_acc: 0.7188\n",
      "Epoch 32/50\n",
      "576/576 [==============================] - 0s 21us/step - loss: 0.5827 - acc: 0.7031 - val_loss: 0.6005 - val_acc: 0.7187\n",
      "Epoch 33/50\n",
      "576/576 [==============================] - 0s 22us/step - loss: 0.5809 - acc: 0.7049 - val_loss: 0.6075 - val_acc: 0.7188\n",
      "Epoch 34/50\n",
      "576/576 [==============================] - 0s 22us/step - loss: 0.5801 - acc: 0.7101 - val_loss: 0.6015 - val_acc: 0.7135\n",
      "Epoch 35/50\n",
      "576/576 [==============================] - 0s 24us/step - loss: 0.5813 - acc: 0.7031 - val_loss: 0.6036 - val_acc: 0.7188\n",
      "Epoch 36/50\n",
      "576/576 [==============================] - 0s 24us/step - loss: 0.5768 - acc: 0.7170 - val_loss: 0.5998 - val_acc: 0.7187\n",
      "Epoch 37/50\n",
      "576/576 [==============================] - 0s 22us/step - loss: 0.5777 - acc: 0.7031 - val_loss: 0.6000 - val_acc: 0.7187\n",
      "Epoch 38/50\n",
      "576/576 [==============================] - 0s 25us/step - loss: 0.5747 - acc: 0.7170 - val_loss: 0.6102 - val_acc: 0.7031\n",
      "Epoch 39/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5735 - acc: 0.7240 - val_loss: 0.6004 - val_acc: 0.7292\n",
      "Epoch 40/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5776 - acc: 0.7170 - val_loss: 0.6020 - val_acc: 0.7188\n",
      "Epoch 41/50\n",
      "576/576 [==============================] - 0s 27us/step - loss: 0.5731 - acc: 0.7101 - val_loss: 0.6009 - val_acc: 0.7188\n",
      "Epoch 42/50\n",
      "576/576 [==============================] - 0s 25us/step - loss: 0.5709 - acc: 0.7153 - val_loss: 0.6016 - val_acc: 0.7188\n",
      "Epoch 43/50\n",
      "576/576 [==============================] - 0s 24us/step - loss: 0.5718 - acc: 0.7170 - val_loss: 0.6000 - val_acc: 0.6979\n",
      "Epoch 44/50\n",
      "576/576 [==============================] - 0s 22us/step - loss: 0.5717 - acc: 0.7083 - val_loss: 0.6015 - val_acc: 0.7188\n",
      "Epoch 45/50\n",
      "576/576 [==============================] - 0s 23us/step - loss: 0.5704 - acc: 0.7187 - val_loss: 0.6050 - val_acc: 0.6979\n",
      "Epoch 46/50\n",
      "576/576 [==============================] - 0s 24us/step - loss: 0.5687 - acc: 0.7083 - val_loss: 0.5982 - val_acc: 0.7240\n",
      "Epoch 47/50\n",
      "576/576 [==============================] - 0s 23us/step - loss: 0.5698 - acc: 0.7170 - val_loss: 0.6010 - val_acc: 0.7135\n",
      "Epoch 48/50\n",
      "576/576 [==============================] - 0s 25us/step - loss: 0.5696 - acc: 0.7118 - val_loss: 0.5997 - val_acc: 0.7031\n",
      "Epoch 49/50\n",
      "576/576 [==============================] - 0s 25us/step - loss: 0.5702 - acc: 0.7205 - val_loss: 0.6110 - val_acc: 0.7135\n",
      "Epoch 50/50\n",
      "576/576 [==============================] - 0s 24us/step - loss: 0.5694 - acc: 0.7066 - val_loss: 0.5989 - val_acc: 0.7135\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "history = model.fit(X_train, # Features\n",
    "                      y_train, # Target\n",
    "                      epochs=50, # Number of epochs\n",
    "                      verbose=1, # Some output, how long its taking and how its performing\n",
    "                      batch_size=50, # Number of observations per batch, how many observations () to pass through and back through with back propogation to update the weights   \n",
    "                      validation_data=(X_test, y_test)) # Data for evaluation\n",
    "#Can look at validation accuracy and see how it continually gets down or bounces around, debating on what you should choose for your batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens when you change the number of neurons per layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Number of Hidden Layers**\n",
    "\n",
    "For many problems you can start with just one or two hidden layers it will work just fine. For more complex problems, you can gradually ramp up the number of hidden layers until your model starts to over fit. \n",
    "\n",
    "Very complex tasks, like image classification, will need dozens of layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-layer network\n",
    "model2 = Sequential()\n",
    "\n",
    "#create first hidden layer\n",
    "model2.add(Dense(12, input_dim=8, activation='relu'))\n",
    "#create second hidden layer\n",
    "model2.add(Dense(8,activation='relu'))\n",
    "#creating third hidden layer (checking out if it made it better or not)\n",
    "model2.add(Dense(13,activation = 'relu'))\n",
    "#Final Layer\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#compile model\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/50\n",
      "576/576 [==============================] - 0s 574us/step - loss: 4.6131 - acc: 0.6528 - val_loss: 3.8124 - val_acc: 0.6250\n",
      "Epoch 2/50\n",
      "576/576 [==============================] - 0s 27us/step - loss: 2.7451 - acc: 0.6094 - val_loss: 2.2917 - val_acc: 0.4740\n",
      "Epoch 3/50\n",
      "576/576 [==============================] - 0s 27us/step - loss: 1.8097 - acc: 0.4861 - val_loss: 1.8056 - val_acc: 0.4271\n",
      "Epoch 4/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 1.5568 - acc: 0.4306 - val_loss: 1.4892 - val_acc: 0.4583\n",
      "Epoch 5/50\n",
      "576/576 [==============================] - 0s 35us/step - loss: 1.3256 - acc: 0.4271 - val_loss: 1.2830 - val_acc: 0.4271\n",
      "Epoch 6/50\n",
      "576/576 [==============================] - 0s 34us/step - loss: 1.1883 - acc: 0.4757 - val_loss: 1.1639 - val_acc: 0.5260\n",
      "Epoch 7/50\n",
      "576/576 [==============================] - 0s 31us/step - loss: 1.1165 - acc: 0.4983 - val_loss: 1.0808 - val_acc: 0.5312\n",
      "Epoch 8/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 1.0419 - acc: 0.4618 - val_loss: 1.0154 - val_acc: 0.5417\n",
      "Epoch 9/50\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.9730 - acc: 0.4844 - val_loss: 0.9586 - val_acc: 0.5729\n",
      "Epoch 10/50\n",
      "576/576 [==============================] - 0s 27us/step - loss: 0.9197 - acc: 0.5087 - val_loss: 0.9131 - val_acc: 0.5573\n",
      "Epoch 11/50\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.8762 - acc: 0.5087 - val_loss: 0.8722 - val_acc: 0.5573\n",
      "Epoch 12/50\n",
      "576/576 [==============================] - 0s 27us/step - loss: 0.8347 - acc: 0.5347 - val_loss: 0.8396 - val_acc: 0.5781\n",
      "Epoch 13/50\n",
      "576/576 [==============================] - 0s 27us/step - loss: 0.8024 - acc: 0.5677 - val_loss: 0.8089 - val_acc: 0.5833\n",
      "Epoch 14/50\n",
      "576/576 [==============================] - 0s 27us/step - loss: 0.7783 - acc: 0.5712 - val_loss: 0.7824 - val_acc: 0.5990\n",
      "Epoch 15/50\n",
      "576/576 [==============================] - 0s 27us/step - loss: 0.7546 - acc: 0.5799 - val_loss: 0.7611 - val_acc: 0.5885\n",
      "Epoch 16/50\n",
      "576/576 [==============================] - 0s 27us/step - loss: 0.7333 - acc: 0.5851 - val_loss: 0.7425 - val_acc: 0.6094\n",
      "Epoch 17/50\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.7227 - acc: 0.6146 - val_loss: 0.7269 - val_acc: 0.6302\n",
      "Epoch 18/50\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.6976 - acc: 0.6319 - val_loss: 0.7138 - val_acc: 0.6406\n",
      "Epoch 19/50\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.6812 - acc: 0.6372 - val_loss: 0.6978 - val_acc: 0.6667\n",
      "Epoch 20/50\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.6653 - acc: 0.6337 - val_loss: 0.6845 - val_acc: 0.6667\n",
      "Epoch 21/50\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.6543 - acc: 0.6528 - val_loss: 0.6716 - val_acc: 0.6667\n",
      "Epoch 22/50\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.6488 - acc: 0.6545 - val_loss: 0.6618 - val_acc: 0.6771\n",
      "Epoch 23/50\n",
      "576/576 [==============================] - 0s 27us/step - loss: 0.6387 - acc: 0.6649 - val_loss: 0.6552 - val_acc: 0.6771\n",
      "Epoch 24/50\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.6328 - acc: 0.6719 - val_loss: 0.6453 - val_acc: 0.6667\n",
      "Epoch 25/50\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.6318 - acc: 0.6719 - val_loss: 0.6444 - val_acc: 0.6823\n",
      "Epoch 26/50\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.6207 - acc: 0.6719 - val_loss: 0.6380 - val_acc: 0.6667\n",
      "Epoch 27/50\n",
      "576/576 [==============================] - 0s 27us/step - loss: 0.6191 - acc: 0.6753 - val_loss: 0.6324 - val_acc: 0.6771\n",
      "Epoch 28/50\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.6166 - acc: 0.6858 - val_loss: 0.6341 - val_acc: 0.6719\n",
      "Epoch 29/50\n",
      "576/576 [==============================] - 0s 27us/step - loss: 0.6120 - acc: 0.6892 - val_loss: 0.6323 - val_acc: 0.6771\n",
      "Epoch 30/50\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.6086 - acc: 0.6927 - val_loss: 0.6317 - val_acc: 0.6875\n",
      "Epoch 31/50\n",
      "576/576 [==============================] - 0s 26us/step - loss: 0.6058 - acc: 0.6997 - val_loss: 0.6309 - val_acc: 0.6823\n",
      "Epoch 32/50\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.6040 - acc: 0.6979 - val_loss: 0.6285 - val_acc: 0.6771\n",
      "Epoch 33/50\n",
      "576/576 [==============================] - 0s 27us/step - loss: 0.6028 - acc: 0.6979 - val_loss: 0.6278 - val_acc: 0.6823\n",
      "Epoch 34/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.6015 - acc: 0.7014 - val_loss: 0.6257 - val_acc: 0.6823\n",
      "Epoch 35/50\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.6014 - acc: 0.6997 - val_loss: 0.6244 - val_acc: 0.6823\n",
      "Epoch 36/50\n",
      "576/576 [==============================] - 0s 27us/step - loss: 0.5989 - acc: 0.7083 - val_loss: 0.6223 - val_acc: 0.6771\n",
      "Epoch 37/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5977 - acc: 0.6997 - val_loss: 0.6224 - val_acc: 0.6823\n",
      "Epoch 38/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5951 - acc: 0.7066 - val_loss: 0.6199 - val_acc: 0.6771\n",
      "Epoch 39/50\n",
      "576/576 [==============================] - 0s 30us/step - loss: 0.5931 - acc: 0.6979 - val_loss: 0.6181 - val_acc: 0.6823\n",
      "Epoch 40/50\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.5915 - acc: 0.6962 - val_loss: 0.6168 - val_acc: 0.6875\n",
      "Epoch 41/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5922 - acc: 0.6962 - val_loss: 0.6162 - val_acc: 0.6823\n",
      "Epoch 42/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5908 - acc: 0.7083 - val_loss: 0.6154 - val_acc: 0.6875\n",
      "Epoch 43/50\n",
      "576/576 [==============================] - 0s 30us/step - loss: 0.5891 - acc: 0.6997 - val_loss: 0.6135 - val_acc: 0.6875\n",
      "Epoch 44/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5856 - acc: 0.7014 - val_loss: 0.6132 - val_acc: 0.6979\n",
      "Epoch 45/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5890 - acc: 0.6979 - val_loss: 0.6134 - val_acc: 0.6875\n",
      "Epoch 46/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5916 - acc: 0.6997 - val_loss: 0.6130 - val_acc: 0.6927\n",
      "Epoch 47/50\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.5862 - acc: 0.7101 - val_loss: 0.6120 - val_acc: 0.6823\n",
      "Epoch 48/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5825 - acc: 0.7153 - val_loss: 0.6155 - val_acc: 0.6875\n",
      "Epoch 49/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5815 - acc: 0.6997 - val_loss: 0.6118 - val_acc: 0.6823\n",
      "Epoch 50/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.5836 - acc: 0.7083 - val_loss: 0.6088 - val_acc: 0.6979\n"
     ]
    }
   ],
   "source": [
    "#train 2nd model\n",
    "history2 = model2.fit(X_train, # Features\n",
    "                      y_train, # Target\n",
    "                      epochs=50, # Number of epochs\n",
    "                      verbose=1, # Some output\n",
    "                      batch_size=50, # Number of observations per batch\n",
    "                      validation_data=(X_test, y_test)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# calculate predictions\n",
    "predictions = model2.predict(X_test)\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens when you change the number of layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network with different activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[Activation Functions](https://towardsdatascience.com/exploring-activation-functions-for-neural-networks-73498da59b02)**\n",
    "    - Linear\n",
    "    - Sigmoid\n",
    "    - Softmax\n",
    "    - Tanh\n",
    "    - ReLu\n",
    "    - elu\n",
    "    \n",
    "In most cases you can use the ReLu activation function (or one of its variants) in the hidden layers. \n",
    "\n",
    "For the output layer, the softmax activation function is generally good for multiclass problems and the sigmouid function for binary classificatin problems. For regression tasks, you can simply use no activation function at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/50\n",
      "576/576 [==============================] - 0s 748us/step - loss: 0.7162 - acc: 0.4028 - val_loss: 0.7018 - val_acc: 0.5156\n",
      "Epoch 2/50\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.6893 - acc: 0.5521 - val_loss: 0.6856 - val_acc: 0.5417\n",
      "Epoch 3/50\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.6658 - acc: 0.6319 - val_loss: 0.6575 - val_acc: 0.6563\n",
      "Epoch 4/50\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.6486 - acc: 0.6615 - val_loss: 0.6468 - val_acc: 0.6719\n",
      "Epoch 5/50\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.6400 - acc: 0.6597 - val_loss: 0.6446 - val_acc: 0.6406\n",
      "Epoch 6/50\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.6418 - acc: 0.6545 - val_loss: 0.6396 - val_acc: 0.6406\n",
      "Epoch 7/50\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.6375 - acc: 0.6545 - val_loss: 0.6389 - val_acc: 0.6406\n",
      "Epoch 8/50\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.6347 - acc: 0.6545 - val_loss: 0.6478 - val_acc: 0.6406\n",
      "Epoch 9/50\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.6311 - acc: 0.6545 - val_loss: 0.6451 - val_acc: 0.6406\n",
      "Epoch 10/50\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.6330 - acc: 0.6545 - val_loss: 0.6374 - val_acc: 0.6406\n",
      "Epoch 11/50\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.6317 - acc: 0.6545 - val_loss: 0.6393 - val_acc: 0.6406\n",
      "Epoch 12/50\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.6327 - acc: 0.6545 - val_loss: 0.6405 - val_acc: 0.6406\n",
      "Epoch 13/50\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.6279 - acc: 0.6545 - val_loss: 0.6473 - val_acc: 0.6406\n",
      "Epoch 14/50\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.6260 - acc: 0.6545 - val_loss: 0.6399 - val_acc: 0.6406\n",
      "Epoch 15/50\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.6278 - acc: 0.6545 - val_loss: 0.6398 - val_acc: 0.6406\n",
      "Epoch 16/50\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.6257 - acc: 0.6545 - val_loss: 0.6412 - val_acc: 0.6406\n",
      "Epoch 17/50\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.6253 - acc: 0.6476 - val_loss: 0.6406 - val_acc: 0.6406\n",
      "Epoch 18/50\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.6249 - acc: 0.6545 - val_loss: 0.6388 - val_acc: 0.6406\n",
      "Epoch 19/50\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.6204 - acc: 0.6632 - val_loss: 0.6362 - val_acc: 0.6823\n",
      "Epoch 20/50\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.6313 - acc: 0.6701 - val_loss: 0.6360 - val_acc: 0.6667\n",
      "Epoch 21/50\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.6327 - acc: 0.6684 - val_loss: 0.6387 - val_acc: 0.6667\n",
      "Epoch 22/50\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.6312 - acc: 0.6684 - val_loss: 0.6362 - val_acc: 0.6667\n",
      "Epoch 23/50\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.6245 - acc: 0.6823 - val_loss: 0.6354 - val_acc: 0.6771\n",
      "Epoch 24/50\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.6220 - acc: 0.6875 - val_loss: 0.6256 - val_acc: 0.7031\n",
      "Epoch 25/50\n",
      "576/576 [==============================] - 0s 100us/step - loss: 0.6195 - acc: 0.6823 - val_loss: 0.6312 - val_acc: 0.6823\n",
      "Epoch 26/50\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.6208 - acc: 0.6858 - val_loss: 0.6284 - val_acc: 0.6927\n",
      "Epoch 27/50\n",
      "576/576 [==============================] - 0s 98us/step - loss: 0.6226 - acc: 0.6806 - val_loss: 0.6346 - val_acc: 0.6719\n",
      "Epoch 28/50\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.6189 - acc: 0.6771 - val_loss: 0.6306 - val_acc: 0.6823\n",
      "Epoch 29/50\n",
      "576/576 [==============================] - 0s 110us/step - loss: 0.6297 - acc: 0.6840 - val_loss: 0.6486 - val_acc: 0.6406\n",
      "Epoch 30/50\n",
      "576/576 [==============================] - 0s 106us/step - loss: 0.6277 - acc: 0.6719 - val_loss: 0.6167 - val_acc: 0.7031\n",
      "Epoch 31/50\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.6194 - acc: 0.6892 - val_loss: 0.6321 - val_acc: 0.6771\n",
      "Epoch 32/50\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.6214 - acc: 0.6892 - val_loss: 0.6226 - val_acc: 0.6771\n",
      "Epoch 33/50\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.6182 - acc: 0.6927 - val_loss: 0.6180 - val_acc: 0.6979\n",
      "Epoch 34/50\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.6207 - acc: 0.6944 - val_loss: 0.6540 - val_acc: 0.6458\n",
      "Epoch 35/50\n",
      "576/576 [==============================] - 0s 128us/step - loss: 0.6315 - acc: 0.6701 - val_loss: 0.6613 - val_acc: 0.6406\n",
      "Epoch 36/50\n",
      "576/576 [==============================] - 0s 198us/step - loss: 0.6228 - acc: 0.6545 - val_loss: 0.6254 - val_acc: 0.6406\n",
      "Epoch 37/50\n",
      "576/576 [==============================] - 0s 165us/step - loss: 0.6164 - acc: 0.6545 - val_loss: 0.6180 - val_acc: 0.6406\n",
      "Epoch 38/50\n",
      "576/576 [==============================] - 0s 114us/step - loss: 0.6127 - acc: 0.6563 - val_loss: 0.6059 - val_acc: 0.6667\n",
      "Epoch 39/50\n",
      "576/576 [==============================] - 0s 108us/step - loss: 0.5980 - acc: 0.6649 - val_loss: 0.6223 - val_acc: 0.6510\n",
      "Epoch 40/50\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.5899 - acc: 0.6910 - val_loss: 0.5984 - val_acc: 0.6927\n",
      "Epoch 41/50\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.5891 - acc: 0.6788 - val_loss: 0.5917 - val_acc: 0.6771\n",
      "Epoch 42/50\n",
      "576/576 [==============================] - 0s 98us/step - loss: 0.5742 - acc: 0.6875 - val_loss: 0.5977 - val_acc: 0.6979\n",
      "Epoch 43/50\n",
      "576/576 [==============================] - 0s 145us/step - loss: 0.6022 - acc: 0.6875 - val_loss: 0.6432 - val_acc: 0.6823\n",
      "Epoch 44/50\n",
      "576/576 [==============================] - 0s 129us/step - loss: 0.6071 - acc: 0.6892 - val_loss: 0.6120 - val_acc: 0.6771\n",
      "Epoch 45/50\n",
      "576/576 [==============================] - 0s 110us/step - loss: 0.5777 - acc: 0.6944 - val_loss: 0.5876 - val_acc: 0.6927\n",
      "Epoch 46/50\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.5628 - acc: 0.6927 - val_loss: 0.5866 - val_acc: 0.6979\n",
      "Epoch 47/50\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.5675 - acc: 0.6979 - val_loss: 0.5936 - val_acc: 0.6927\n",
      "Epoch 48/50\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.5712 - acc: 0.6927 - val_loss: 0.5983 - val_acc: 0.6823\n",
      "Epoch 49/50\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.5737 - acc: 0.6962 - val_loss: 0.5829 - val_acc: 0.6927\n",
      "Epoch 50/50\n",
      "576/576 [==============================] - 0s 98us/step - loss: 0.5617 - acc: 0.6927 - val_loss: 0.5872 - val_acc: 0.6979\n"
     ]
    }
   ],
   "source": [
    "#3-layer network with sigmoid activation function\n",
    "model3 = Sequential()\n",
    "\n",
    "#create first hidden layer\n",
    "model3.add(Dense(20, input_dim=8, activation='elu'))\n",
    "#create second hidden layer\n",
    "model3.add(Dense(10,activation='softmax'))\n",
    "#create third hidden layer\n",
    "model3.add(Dense(5,activation='relu'))\n",
    "#Final Layer\n",
    "model3.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "#compile model\n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#train 2nd model\n",
    "history3 = model3.fit(X_train, # Features\n",
    "                      y_train, # Target\n",
    "                      epochs=50, # Number of epochs\n",
    "                      verbose=1, # Some output\n",
    "                      batch_size=10, # Number of observations per batch\n",
    "                      validation_data=(X_test, y_test)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens when you change the activation functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network with different optimizer and learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[Selecting an optimizer](https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/)**\n",
    "    - Adam\n",
    "    - SGD\n",
    "    - RMSprop\n",
    "    - Adagrad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Learning Rate**\n",
    "\n",
    "    - If you set it too low, training will eventually converge, but it will do so slowly.\n",
    "\n",
    "    - If you set it too high, it might acutally diverge.\n",
    "\n",
    "    - If you set it slightly too high, it will converge at first but miss the local optima.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/50\n",
      "576/576 [==============================] - 0s 856us/step - loss: 0.6959 - acc: 0.6337 - val_loss: 0.6558 - val_acc: 0.6406\n",
      "Epoch 2/50\n",
      "576/576 [==============================] - 0s 30us/step - loss: 0.6476 - acc: 0.6545 - val_loss: 0.6843 - val_acc: 0.6406\n",
      "Epoch 3/50\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.6519 - acc: 0.6545 - val_loss: 0.6595 - val_acc: 0.6406\n",
      "Epoch 4/50\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.6495 - acc: 0.6267 - val_loss: 0.6544 - val_acc: 0.6406\n",
      "Epoch 5/50\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.6509 - acc: 0.6545 - val_loss: 0.6733 - val_acc: 0.6406\n",
      "Epoch 6/50\n",
      "576/576 [==============================] - 0s 30us/step - loss: 0.6509 - acc: 0.6545 - val_loss: 0.6533 - val_acc: 0.6406\n",
      "Epoch 7/50\n",
      "576/576 [==============================] - 0s 30us/step - loss: 0.6508 - acc: 0.6545 - val_loss: 0.6709 - val_acc: 0.6406\n",
      "Epoch 8/50\n",
      "576/576 [==============================] - 0s 32us/step - loss: 0.6494 - acc: 0.6545 - val_loss: 0.6576 - val_acc: 0.6406\n",
      "Epoch 9/50\n",
      "576/576 [==============================] - 0s 30us/step - loss: 0.6487 - acc: 0.6545 - val_loss: 0.6634 - val_acc: 0.6406\n",
      "Epoch 10/50\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.6495 - acc: 0.6545 - val_loss: 0.6716 - val_acc: 0.6406\n",
      "Epoch 11/50\n",
      "576/576 [==============================] - 0s 30us/step - loss: 0.6522 - acc: 0.6545 - val_loss: 0.6678 - val_acc: 0.6406\n",
      "Epoch 12/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.6488 - acc: 0.6545 - val_loss: 0.6567 - val_acc: 0.6406\n",
      "Epoch 13/50\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.6474 - acc: 0.6545 - val_loss: 0.6540 - val_acc: 0.6406\n",
      "Epoch 14/50\n",
      "576/576 [==============================] - 0s 30us/step - loss: 0.6464 - acc: 0.6545 - val_loss: 0.6573 - val_acc: 0.6406\n",
      "Epoch 15/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.6489 - acc: 0.6545 - val_loss: 0.6582 - val_acc: 0.6406\n",
      "Epoch 16/50\n",
      "576/576 [==============================] - 0s 30us/step - loss: 0.6455 - acc: 0.6545 - val_loss: 0.6598 - val_acc: 0.6406\n",
      "Epoch 17/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.6485 - acc: 0.6545 - val_loss: 0.6535 - val_acc: 0.6406\n",
      "Epoch 18/50\n",
      "576/576 [==============================] - 0s 27us/step - loss: 0.6511 - acc: 0.6545 - val_loss: 0.6543 - val_acc: 0.6406\n",
      "Epoch 19/50\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.6465 - acc: 0.6545 - val_loss: 0.6630 - val_acc: 0.6406\n",
      "Epoch 20/50\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.6477 - acc: 0.6545 - val_loss: 0.6532 - val_acc: 0.6406\n",
      "Epoch 21/50\n",
      "576/576 [==============================] - 0s 27us/step - loss: 0.6470 - acc: 0.6545 - val_loss: 0.6533 - val_acc: 0.6406\n",
      "Epoch 22/50\n",
      "576/576 [==============================] - 0s 27us/step - loss: 0.6448 - acc: 0.6545 - val_loss: 0.6603 - val_acc: 0.6406\n",
      "Epoch 23/50\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.6481 - acc: 0.6545 - val_loss: 0.6538 - val_acc: 0.6406\n",
      "Epoch 24/50\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.6456 - acc: 0.6545 - val_loss: 0.6581 - val_acc: 0.6406\n",
      "Epoch 25/50\n",
      "576/576 [==============================] - 0s 27us/step - loss: 0.6497 - acc: 0.6545 - val_loss: 0.6537 - val_acc: 0.6406\n",
      "Epoch 26/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.6478 - acc: 0.6545 - val_loss: 0.6544 - val_acc: 0.6406\n",
      "Epoch 27/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.6455 - acc: 0.6545 - val_loss: 0.6533 - val_acc: 0.6406\n",
      "Epoch 28/50\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.6486 - acc: 0.6545 - val_loss: 0.6536 - val_acc: 0.6406\n",
      "Epoch 29/50\n",
      "576/576 [==============================] - 0s 34us/step - loss: 0.6463 - acc: 0.6545 - val_loss: 0.6554 - val_acc: 0.6406\n",
      "Epoch 30/50\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.6462 - acc: 0.6545 - val_loss: 0.6615 - val_acc: 0.6406\n",
      "Epoch 31/50\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.6485 - acc: 0.6545 - val_loss: 0.6531 - val_acc: 0.6406\n",
      "Epoch 32/50\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.6463 - acc: 0.6545 - val_loss: 0.6535 - val_acc: 0.6406\n",
      "Epoch 33/50\n",
      "576/576 [==============================] - 0s 32us/step - loss: 0.6464 - acc: 0.6545 - val_loss: 0.6552 - val_acc: 0.6406\n",
      "Epoch 34/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.6478 - acc: 0.6545 - val_loss: 0.6531 - val_acc: 0.6406\n",
      "Epoch 35/50\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.6474 - acc: 0.6545 - val_loss: 0.6553 - val_acc: 0.6406\n",
      "Epoch 36/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 0.6465 - acc: 0.6545 - val_loss: 0.6533 - val_acc: 0.6406\n",
      "Epoch 37/50\n",
      "576/576 [==============================] - 0s 32us/step - loss: 0.6465 - acc: 0.6545 - val_loss: 0.6593 - val_acc: 0.6406\n",
      "Epoch 38/50\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.6489 - acc: 0.6545 - val_loss: 0.6534 - val_acc: 0.6406\n",
      "Epoch 39/50\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.6454 - acc: 0.6545 - val_loss: 0.6580 - val_acc: 0.6406\n",
      "Epoch 40/50\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.6489 - acc: 0.6545 - val_loss: 0.6531 - val_acc: 0.6406\n",
      "Epoch 41/50\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.6463 - acc: 0.6545 - val_loss: 0.6533 - val_acc: 0.6406\n",
      "Epoch 42/50\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.6455 - acc: 0.6545 - val_loss: 0.6544 - val_acc: 0.6406\n",
      "Epoch 43/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.6464 - acc: 0.6545 - val_loss: 0.6557 - val_acc: 0.6406\n",
      "Epoch 44/50\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.6459 - acc: 0.6545 - val_loss: 0.6537 - val_acc: 0.6406\n",
      "Epoch 45/50\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.6469 - acc: 0.6545 - val_loss: 0.6566 - val_acc: 0.6406\n",
      "Epoch 46/50\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.6472 - acc: 0.6545 - val_loss: 0.6552 - val_acc: 0.6406\n",
      "Epoch 47/50\n",
      "576/576 [==============================] - 0s 34us/step - loss: 0.6471 - acc: 0.6545 - val_loss: 0.6545 - val_acc: 0.6406\n",
      "Epoch 48/50\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.6497 - acc: 0.6545 - val_loss: 0.6531 - val_acc: 0.6406\n",
      "Epoch 49/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.6459 - acc: 0.6545 - val_loss: 0.6532 - val_acc: 0.6406\n",
      "Epoch 50/50\n",
      "576/576 [==============================] - 0s 34us/step - loss: 0.6485 - acc: 0.6545 - val_loss: 0.6533 - val_acc: 0.6406\n"
     ]
    }
   ],
   "source": [
    "#2-layer network with rmsprop optimizer and a learning rate of .1\n",
    "model4 = Sequential()\n",
    "\n",
    "#create first hidden layer\n",
    "model4.add(Dense(10, input_dim=8, activation='sigmoid'))\n",
    "#create second hidden layer\n",
    "model4.add(Dense(10,activation='sigmoid'))\n",
    "#Final Layer\n",
    "model4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#compile model, Where we set our learning rate, which is multiplied to our gradient to decide how far of a step to take in finding the minimum in our loss function\n",
    "rmsprop = keras.optimizers.RMSprop(lr=0.1, rho=0.9, epsilon=None, decay=0.0)\n",
    "model4.compile(loss='binary_crossentropy', optimizer=rmsprop, metrics=['accuracy'])\n",
    "\n",
    "#train 2nd model\n",
    "history4 = model4.fit(X_train, # Features\n",
    "                      y_train, # Target\n",
    "                      epochs=50, # Number of epochs\n",
    "                      verbose=1, # Some output\n",
    "                      batch_size=50, # Number of observations per batch\n",
    "                      validation_data=(X_test, y_test)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens when you change the optimizer and learning rates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Regularization\n",
    "    - L1 and L2\n",
    "\n",
    "    - Dropout \n",
    "        \n",
    "        The most popular techniqure for deep neural networks. It is a fairly simple algorithm where at every training step, every neuron has a probability fo being teporarily \"droppedout,\" meaning it will be completely ignored dureing this traing step, but it may be active during the next step.\n",
    "    \n",
    "    - [Early Stopping](https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/)\n",
    "    \n",
    "   Just interrupt training when its performance on the validation set starts dropping\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Paper on selecting hyperparameters](https://arxiv.org/pdf/1206.5533v2.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0711 11:31:49.799200 4629296576 deprecation.py:506] From /Users/kalkidanlemma/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/50\n",
      "576/576 [==============================] - 1s 1ms/step - loss: 6.3082 - acc: 0.4201 - val_loss: 5.1660 - val_acc: 0.3490\n",
      "Epoch 2/50\n",
      "576/576 [==============================] - 0s 29us/step - loss: 5.8950 - acc: 0.3924 - val_loss: 4.1565 - val_acc: 0.3438\n",
      "Epoch 3/50\n",
      "576/576 [==============================] - 0s 32us/step - loss: 5.3127 - acc: 0.4358 - val_loss: 3.2187 - val_acc: 0.3646\n",
      "Epoch 4/50\n",
      "576/576 [==============================] - 0s 36us/step - loss: 4.9783 - acc: 0.4132 - val_loss: 2.4127 - val_acc: 0.3594\n",
      "Epoch 5/50\n",
      "576/576 [==============================] - 0s 36us/step - loss: 3.4417 - acc: 0.4792 - val_loss: 1.6986 - val_acc: 0.3646\n",
      "Epoch 6/50\n",
      "576/576 [==============================] - 0s 35us/step - loss: 3.6284 - acc: 0.4757 - val_loss: 1.2656 - val_acc: 0.5104\n",
      "Epoch 7/50\n",
      "576/576 [==============================] - 0s 35us/step - loss: 2.9786 - acc: 0.5104 - val_loss: 1.1059 - val_acc: 0.6094\n",
      "Epoch 8/50\n",
      "576/576 [==============================] - 0s 38us/step - loss: 2.4330 - acc: 0.5122 - val_loss: 1.0298 - val_acc: 0.6146\n",
      "Epoch 9/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 2.2846 - acc: 0.5295 - val_loss: 0.9965 - val_acc: 0.6354\n",
      "Epoch 10/50\n",
      "576/576 [==============================] - 0s 34us/step - loss: 1.7735 - acc: 0.5625 - val_loss: 0.9716 - val_acc: 0.6354\n",
      "Epoch 11/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 1.9081 - acc: 0.5642 - val_loss: 0.9348 - val_acc: 0.6406\n",
      "Epoch 12/50\n",
      "576/576 [==============================] - 0s 32us/step - loss: 1.7406 - acc: 0.5608 - val_loss: 0.9053 - val_acc: 0.6406\n",
      "Epoch 13/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 1.6071 - acc: 0.5382 - val_loss: 0.8794 - val_acc: 0.6406\n",
      "Epoch 14/50\n",
      "576/576 [==============================] - 0s 31us/step - loss: 1.3394 - acc: 0.5608 - val_loss: 0.8459 - val_acc: 0.6406\n",
      "Epoch 15/50\n",
      "576/576 [==============================] - 0s 32us/step - loss: 1.4906 - acc: 0.5278 - val_loss: 0.8231 - val_acc: 0.6406\n",
      "Epoch 16/50\n",
      "576/576 [==============================] - 0s 32us/step - loss: 1.1765 - acc: 0.5955 - val_loss: 0.8054 - val_acc: 0.6406\n",
      "Epoch 17/50\n",
      "576/576 [==============================] - 0s 32us/step - loss: 1.2348 - acc: 0.5694 - val_loss: 0.7991 - val_acc: 0.6406\n",
      "Epoch 18/50\n",
      "576/576 [==============================] - 0s 34us/step - loss: 1.1064 - acc: 0.6111 - val_loss: 0.7939 - val_acc: 0.6406\n",
      "Epoch 19/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 1.1298 - acc: 0.5972 - val_loss: 0.7851 - val_acc: 0.6406\n",
      "Epoch 20/50\n",
      "576/576 [==============================] - 0s 31us/step - loss: 1.2504 - acc: 0.6128 - val_loss: 0.7798 - val_acc: 0.6406\n",
      "Epoch 21/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 1.1264 - acc: 0.6372 - val_loss: 0.7784 - val_acc: 0.6406\n",
      "Epoch 22/50\n",
      "576/576 [==============================] - 0s 31us/step - loss: 0.9806 - acc: 0.6389 - val_loss: 0.7772 - val_acc: 0.6406\n",
      "Epoch 23/50\n",
      "576/576 [==============================] - 0s 32us/step - loss: 1.0179 - acc: 0.6215 - val_loss: 0.7762 - val_acc: 0.6406\n",
      "Epoch 24/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 1.0538 - acc: 0.6354 - val_loss: 0.7738 - val_acc: 0.6406\n",
      "Epoch 25/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.9626 - acc: 0.6285 - val_loss: 0.7708 - val_acc: 0.6406\n",
      "Epoch 26/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.9569 - acc: 0.6128 - val_loss: 0.7685 - val_acc: 0.6406\n",
      "Epoch 27/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 1.0262 - acc: 0.6441 - val_loss: 0.7662 - val_acc: 0.6406\n",
      "Epoch 28/50\n",
      "576/576 [==============================] - 0s 32us/step - loss: 0.8514 - acc: 0.6337 - val_loss: 0.7654 - val_acc: 0.6406\n",
      "Epoch 29/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.9941 - acc: 0.6181 - val_loss: 0.7644 - val_acc: 0.6406\n",
      "Epoch 30/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.9281 - acc: 0.6024 - val_loss: 0.7621 - val_acc: 0.6406\n",
      "Epoch 31/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.9428 - acc: 0.6441 - val_loss: 0.7611 - val_acc: 0.6406\n",
      "Epoch 32/50\n",
      "576/576 [==============================] - 0s 32us/step - loss: 0.8222 - acc: 0.6632 - val_loss: 0.7606 - val_acc: 0.6406\n",
      "Epoch 33/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.9247 - acc: 0.6198 - val_loss: 0.7599 - val_acc: 0.6406\n",
      "Epoch 34/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.9373 - acc: 0.6302 - val_loss: 0.7598 - val_acc: 0.6406\n",
      "Epoch 35/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.8516 - acc: 0.6406 - val_loss: 0.7588 - val_acc: 0.6406\n",
      "Epoch 36/50\n",
      "576/576 [==============================] - 0s 37us/step - loss: 0.8241 - acc: 0.6476 - val_loss: 0.7571 - val_acc: 0.6406\n",
      "Epoch 37/50\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.7841 - acc: 0.660 - 0s 35us/step - loss: 0.8747 - acc: 0.6580 - val_loss: 0.7566 - val_acc: 0.6406\n",
      "Epoch 38/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.8224 - acc: 0.6372 - val_loss: 0.7560 - val_acc: 0.6406\n",
      "Epoch 39/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.7747 - acc: 0.6476 - val_loss: 0.7561 - val_acc: 0.6406\n",
      "Epoch 40/50\n",
      "576/576 [==============================] - 0s 34us/step - loss: 0.8462 - acc: 0.6441 - val_loss: 0.7551 - val_acc: 0.6406\n",
      "Epoch 41/50\n",
      "576/576 [==============================] - 0s 34us/step - loss: 0.8205 - acc: 0.6424 - val_loss: 0.7534 - val_acc: 0.6406\n",
      "Epoch 42/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.8381 - acc: 0.6406 - val_loss: 0.7523 - val_acc: 0.6406\n",
      "Epoch 43/50\n",
      "576/576 [==============================] - 0s 34us/step - loss: 0.7892 - acc: 0.6441 - val_loss: 0.7508 - val_acc: 0.6406\n",
      "Epoch 44/50\n",
      "576/576 [==============================] - 0s 35us/step - loss: 0.8643 - acc: 0.6389 - val_loss: 0.7496 - val_acc: 0.6406\n",
      "Epoch 45/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.7866 - acc: 0.6441 - val_loss: 0.7490 - val_acc: 0.6406\n",
      "Epoch 46/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.7919 - acc: 0.6424 - val_loss: 0.7487 - val_acc: 0.6406\n",
      "Epoch 47/50\n",
      "576/576 [==============================] - 0s 32us/step - loss: 0.8039 - acc: 0.6545 - val_loss: 0.7481 - val_acc: 0.6406\n",
      "Epoch 48/50\n",
      "576/576 [==============================] - 0s 34us/step - loss: 0.8204 - acc: 0.6476 - val_loss: 0.7478 - val_acc: 0.6406\n",
      "Epoch 49/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.7705 - acc: 0.6563 - val_loss: 0.7470 - val_acc: 0.6406\n",
      "Epoch 50/50\n",
      "576/576 [==============================] - 0s 33us/step - loss: 0.7621 - acc: 0.6597 - val_loss: 0.7459 - val_acc: 0.6406\n"
     ]
    }
   ],
   "source": [
    "#3-layer network with sigmoid activation function\n",
    "model5 = Sequential()\n",
    "\n",
    "#create first hidden layer\n",
    "model5.add(Dense(16, input_dim=8, activation='relu'))\n",
    "# Add a dropout layer for previous hidden layer\n",
    "model5.add(Dropout(0.50))\n",
    "# Add fully connected layer with a ReLU activation function and L2 regularization\n",
    "model5.add(Dense(units=8, kernel_regularizer=regularizers.l2(0.01),activation='relu'))\n",
    "# Add a dropout layer for previous hidden layer\n",
    "model5.add(Dropout(0.5))\n",
    "# Add fully connected layer with a ReLU activation function and L2 regularization\n",
    "model5.add(Dense(units=4, kernel_regularizer=regularizers.l2(0.01),activation='relu'))\n",
    "#Final Layer\n",
    "model5.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "#compile model\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model5.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "#train 2nd model\n",
    "history5 = model5.fit(X_train, # Features\n",
    "                      y_train, # Target\n",
    "                      epochs=50, # Number of epochs\n",
    "                      verbose=1, # Some output\n",
    "                      batch_size=50, # Number of observations per batch\n",
    "                      validation_data=(X_test, y_test)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Using GridSearchCV to tune Neural Networks](https://chrisalbon.com/deep_learning/keras/tuning_neural_network_hyperparameters/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Network Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda install -c anaconda pydot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "`pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1914\u001b[0m                 \u001b[0marguments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1915\u001b[0;31m                 \u001b[0mworking_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pydot.py\u001b[0m in \u001b[0;36mcall_graphviz\u001b[0;34m(program, arguments, working_dir, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    776\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1521\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1523\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dot': 'dot'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     prog=prog)\n\u001b[0;32m-> 1922\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1923\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] \"dot\" not found in path.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-26ba0b8fb20b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Visualize network architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mSVG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'svg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         raise OSError(\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0;34m'`pydot` failed to call GraphViz.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;34m'Please install GraphViz (https://www.graphviz.org/) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             'and ensure that its executables are in the $PATH.')\n",
      "\u001b[0;31mOSError\u001b[0m: `pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH."
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "#Visualize network architecture\n",
    "SVG(model_to_dot(model5, show_shapes=True).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the visualization as a file\n",
    "plot_model(model5, show_shapes=True, to_file='network.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chrisalbon.com/deep_learning/keras/visualize_neural_network_architecture/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Keras Implementation of optimizers](https://keras.io/optimizers/)\n",
    "\n",
    "[Impact of Learning Rate on Model Performance](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate predictions\n",
    "predictions = model5.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_loss = history5.history['loss']\n",
    "test_loss = history5.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chrisalbon.com/deep_learning/keras/visualize_loss_history/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-bb54cf484908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Visualize accuracy history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r--'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Training Accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Test Accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Get training and test accuracy histories\n",
    "training_accuracy = history5.history['acc']\n",
    "test_accuracy = history5.history['val_acc']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "# Visualize accuracy history\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, test_accuracy, 'b-')\n",
    "plt.legend(['Training Accuracy', 'Test Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chrisalbon.com/deep_learning/keras/visualize_performance_history/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# calculate predictions\n",
    "predictions = model5.predict(X)\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources \n",
    "\n",
    "http://neuralnetworksanddeeplearning.com/\n",
    "    \n",
    "http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\n",
    "\n",
    "https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
